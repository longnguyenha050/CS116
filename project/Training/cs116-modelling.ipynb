{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.11.11","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"none","dataSources":[{"sourceId":10211,"databundleVersionId":111096,"sourceType":"competition"}],"dockerImageVersionId":31012,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":false}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"import numpy as np  # linear algebra\nimport pandas as pd  #\nfrom datetime import datetime\n\nfrom scipy.stats import skew  # for some statistics\nfrom scipy.special import boxcox1p\nfrom scipy.stats import boxcox_normmax\n\nfrom sklearn.linear_model import ElasticNetCV, LassoCV, RidgeCV\nfrom sklearn.ensemble import GradientBoostingRegressor\nfrom sklearn.svm import SVR\nfrom sklearn.pipeline import make_pipeline\nfrom sklearn.preprocessing import RobustScaler\nfrom sklearn.model_selection import KFold, cross_val_score\nfrom sklearn.metrics import mean_squared_error, mean_absolute_error\n\nfrom mlxtend.regressor import StackingCVRegressor\n\nfrom xgboost import XGBRegressor\nfrom lightgbm import LGBMRegressor\n\nimport os\n\nprint(os.listdir(\"../input\"))\n\ntrain = pd.read_csv('/kaggle/input/home-data-for-ml-course/train.csv')\ntest = pd.read_csv('/kaggle/input/home-data-for-ml-course/test.csv')\nprint(\"Train set size:\", train.shape)\nprint(\"Test set size:\", test.shape)\nprint('START data processing', datetime.now(), )\n\ntrain_ID = train['Id']\ntest_ID = test['Id']\n# Now drop the  'Id' colum since it's unnecessary for  the prediction process.\ntrain.drop(['Id'], axis=1, inplace=True)\ntest.drop(['Id'], axis=1, inplace=True)\n\n# Deleting outliers\ntrain = train[train.GrLivArea < 4500]\ntrain.reset_index(drop=True, inplace=True)\n\n# We use the numpy fuction log1p which  applies log(1+x) to all elements of the column\ntrain[\"SalePrice\"] = np.log1p(train[\"SalePrice\"])\ny = train.SalePrice.reset_index(drop=True)\ntrain_features = train.drop(['SalePrice'], axis=1)\ntest_features = test\n\nfeatures = pd.concat([train_features, test_features]).reset_index(drop=True)\nprint(features.shape)\n# Some of the non-numeric predictors are stored as numbers; we convert them into strings \nfeatures['MSSubClass'] = features['MSSubClass'].apply(str)\nfeatures['YrSold'] = features['YrSold'].astype(str)\nfeatures['MoSold'] = features['MoSold'].astype(str)\n\nfeatures['Functional'] = features['Functional'].fillna('Typ')\nfeatures['Electrical'] = features['Electrical'].fillna(\"SBrkr\")\nfeatures['KitchenQual'] = features['KitchenQual'].fillna(\"TA\")\nfeatures['Exterior1st'] = features['Exterior1st'].fillna(features['Exterior1st'].mode()[0])\nfeatures['Exterior2nd'] = features['Exterior2nd'].fillna(features['Exterior2nd'].mode()[0])\nfeatures['SaleType'] = features['SaleType'].fillna(features['SaleType'].mode()[0])\n\nfeatures[\"PoolQC\"] = features[\"PoolQC\"].fillna(\"None\")\n\nfor col in ('GarageYrBlt', 'GarageArea', 'GarageCars'):\n    features[col] = features[col].fillna(0)\nfor col in ['GarageType', 'GarageFinish', 'GarageQual', 'GarageCond']:\n    features[col] = features[col].fillna('None')\nfor col in ('BsmtQual', 'BsmtCond', 'BsmtExposure', 'BsmtFinType1', 'BsmtFinType2'):\n    features[col] = features[col].fillna('None')\n\nfeatures['MSZoning'] = features.groupby('MSSubClass')['MSZoning'].transform(lambda x: x.fillna(x.mode()[0]))\n\nobjects = []\nfor i in features.columns:\n    if features[i].dtype == object:\n        objects.append(i)\n\nfeatures.update(features[objects].fillna('None'))\n\nfeatures['LotFrontage'] = features.groupby('Neighborhood')['LotFrontage'].transform(lambda x: x.fillna(x.median()))\n\n# Filling in the rest of the NA's\n\nnumeric_dtypes = ['int16', 'int32', 'int64', 'float16', 'float32', 'float64']\nnumerics = []\nfor i in features.columns:\n    if features[i].dtype in numeric_dtypes:\n        numerics.append(i)\nfeatures.update(features[numerics].fillna(0))\n\nnumeric_dtypes = ['int16', 'int32', 'int64', 'float16', 'float32', 'float64']\nnumerics2 = []\nfor i in features.columns:\n    if features[i].dtype in numeric_dtypes:\n        numerics2.append(i)\n\nskew_features = features[numerics2].apply(lambda x: skew(x)).sort_values(ascending=False)\n\nhigh_skew = skew_features[skew_features > 0.5]\nskew_index = high_skew.index\n\nfor i in skew_index:\n    try:\n        # Try Box-Cox with a small constant\n        features[i] = boxcox1p(features[i], boxcox_normmax(features[i] + 1e-6))\n    except Exception as e:\n        print(f\"Box-Cox failed for column: {i}, error: {e}\")\n        try:\n            # Try Yeo-Johnson transformation\n            transformed_data, lambda_val = yeojohnson(features[i])\n            features[i] = transformed_data\n            print(f\"Yeo-Johnson applied to column: {i}, lambda: {lambda_val}\")\n        except Exception as e_yj:\n            print(f\"Yeo-Johnson also failed for column: {i}, error: {e_yj}\")\n            # Handle the column that fails both transformations\n            # You might choose to skip transformation, apply a different method, or investigate further\n            print(f\"Skipping transformation for column: {i}\")\n\nfeatures['YrBltAndRemod'] = features['YearBuilt'] + features['YearRemodAdd']\nfeatures['TotalSF'] = features['TotalBsmtSF'] + features['1stFlrSF'] + features['2ndFlrSF']\n\nfeatures['Total_sqr_footage'] = (features['BsmtFinSF1'] + features['BsmtFinSF2'] +\n                                 features['1stFlrSF'] + features['2ndFlrSF'])\n\nfeatures['Total_Bathrooms'] = (features['FullBath'] + (0.5 * features['HalfBath']) +\n                               features['BsmtFullBath'] + (0.5 * features['BsmtHalfBath']))\n\nfeatures['Total_porch_sf'] = (features['OpenPorchSF'] + features['3SsnPorch'] +\n                              features['EnclosedPorch'] + features['ScreenPorch'] +\n                              features['WoodDeckSF'])\n\n# simplified features\nfeatures['haspool'] = features['PoolArea'].apply(lambda x: 1 if x > 0 else 0)\nfeatures['has2ndfloor'] = features['2ndFlrSF'].apply(lambda x: 1 if x > 0 else 0)\nfeatures['hasgarage'] = features['GarageArea'].apply(lambda x: 1 if x > 0 else 0)\nfeatures['hasbsmt'] = features['TotalBsmtSF'].apply(lambda x: 1 if x > 0 else 0)\nfeatures['hasfireplace'] = features['Fireplaces'].apply(lambda x: 1 if x > 0 else 0)\n\nprint(features.shape)\nfinal_features = pd.get_dummies(features).reset_index(drop=True)\nprint(final_features.shape)\n\nX = final_features.iloc[:len(y), :]\nX_sub = final_features.iloc[len(X):, :]\n\nprint('X', X.shape, 'y', y.shape, 'X_sub', X_sub.shape)\n\noutliers = [30, 88, 462, 631, 1322]\nX = X.drop(X.index[outliers])\ny = y.drop(y.index[outliers])\n\noverfit = []\nfor i in X.columns:\n    counts = X[i].value_counts()\n    zeros = counts.iloc[0]\n    if zeros / len(X) * 100 > 99.94:\n        overfit.append(i)\n\noverfit = list(overfit)\noverfit.append('MSZoning_C (all)')\n\nX = X.drop(overfit, axis=1).copy()\nX_sub = X_sub.drop(overfit, axis=1).copy()\n\nprint('X', X.shape, 'y', y.shape, 'X_sub', X_sub.shape)\n\n# ################## ML ########################################\nprint('START ML', datetime.now(), )\n\nkfolds = KFold(n_splits=10, shuffle=True, random_state=42)\n\n\n# rmsle\ndef rmsle(y, y_pred):\n    return np.sqrt(mean_squared_error(y, y_pred))\n\n\n# build our model scoring function\ndef cv_rmse(model):\n    rmse = np.sqrt(-cross_val_score(model, X, y,\n                                    scoring=\"neg_mean_squared_error\",\n                                    cv=kfolds))\n    return rmse\n\n\n# setup models    \nalphas_alt = [14.5, 14.6, 14.7, 14.8, 14.9, 15, 15.1, 15.2, 15.3, 15.4, 15.5]\nalphas2 = [5e-05, 0.0001, 0.0002, 0.0003, 0.0004, 0.0005, 0.0006, 0.0007, 0.0008]\ne_alphas = [0.0001, 0.0002, 0.0003, 0.0004, 0.0005, 0.0006, 0.0007]\ne_l1ratio = [0.8, 0.85, 0.9, 0.95, 0.99, 1]\n\nridge = make_pipeline(RobustScaler(),\n                      RidgeCV(alphas=alphas_alt, cv=kfolds))\n\nlasso = make_pipeline(RobustScaler(),\n                     LassoCV(max_iter=int(1e7), alphas=alphas2,\n                             random_state=42, cv=kfolds))\n\nelasticnet = make_pipeline(RobustScaler(),\n                           ElasticNetCV(max_iter=int(1e7), alphas=e_alphas,\n                                        cv=kfolds, l1_ratio=e_l1ratio))\n\nsvr = make_pipeline(RobustScaler(),\n                    SVR(C=20, epsilon=0.008, gamma=0.0003, ))\n\ngbr = GradientBoostingRegressor(n_estimators=3000, learning_rate=0.05,\n                                max_depth=4, max_features='sqrt',\n                                min_samples_leaf=15, min_samples_split=10,\n                                loss='huber', random_state=42)\n\nlightgbm = LGBMRegressor(objective='regression',\n                         num_leaves=4,\n                         learning_rate=0.01,\n                         n_estimators=5000,\n                         max_bin=200,\n                         bagging_fraction=0.75,\n                         bagging_freq=5,\n                         bagging_seed=7,\n                         feature_fraction=0.2,\n                         feature_fraction_seed=7,\n                         verbose=-1,\n                         # min_data_in_leaf=2,\n                         # min_sum_hessian_in_leaf=11\n                         )\n\nxgboost = XGBRegressor(learning_rate=0.01, n_estimators=3460,\n                       max_depth=3, min_child_weight=0,\n                       gamma=0, subsample=0.7,\n                       colsample_bytree=0.7,\n                       objective='reg:linear', nthread=-1,\n                       scale_pos_weight=1, seed=27,\n                       reg_alpha=0.00006)\n\n# stack\nstack_gen = StackingCVRegressor(regressors=(ridge, lasso, elasticnet,\n                                            gbr, xgboost, lightgbm),\n                                meta_regressor=xgboost,\n                                use_features_in_secondary=True)\n\nprint('TEST score on CV')\n\nscore = cv_rmse(ridge)\nprint(\"Kernel Ridge score: {:.4f} ({:.4f})\\n\".format(score.mean(), score.std()), datetime.now(), )\n\nscore = cv_rmse(lasso)\nprint(\"Lasso score: {:.4f} ({:.4f})\\n\".format(score.mean(), score.std()), datetime.now(), )\n\nscore = cv_rmse(elasticnet)\nprint(\"ElasticNet score: {:.4f} ({:.4f})\\n\".format(score.mean(), score.std()), datetime.now(), )\n\nscore = cv_rmse(svr)\nprint(\"SVR score: {:.4f} ({:.4f})\\n\".format(score.mean(), score.std()), datetime.now(), )\n\nscore = cv_rmse(gbr)\nprint(\"GradientBoosting score: {:.4f} ({:.4f})\\n\".format(score.mean(), score.std()), datetime.now(), )\n\nscore = cv_rmse(lightgbm)\nprint(\"Lightgbm score: {:.4f} ({:.4f})\\n\".format(score.mean(), score.std()), datetime.now(), )\n\nscore = cv_rmse(xgboost)\nprint(\"Xgboost score: {:.4f} ({:.4f})\\n\".format(score.mean(), score.std()), datetime.now(), )\n\nprint('START Fit')\nprint(datetime.now(), 'StackingCVRegressor')\nstack_gen_model = stack_gen.fit(np.array(X), np.array(y))\nprint(datetime.now(), 'elasticnet')\nelastic_model_full_data = elasticnet.fit(X, y)\nprint(datetime.now(), 'lasso')\nlasso_model_full_data = lasso.fit(X, y)\nprint(datetime.now(), 'ridge')\nridge_model_full_data = ridge.fit(X, y)\nprint(datetime.now(), 'svr')\nsvr_model_full_data = svr.fit(X, y)\nprint(datetime.now(), 'GradientBoosting')\ngbr_model_full_data = gbr.fit(X, y)\nprint(datetime.now(), 'xgboost')\nxgb_model_full_data = xgboost.fit(X, y)\nprint(datetime.now(), 'lightgbm')\nlgb_model_full_data = lightgbm.fit(X, y)\n\n\ndef blend_models_predict(X=X):\n    return ((0.1* elastic_model_full_data.predict(X)) + \n            (0.1 * lasso_model_full_data.predict(X)) + \n            (0.05 * ridge_model_full_data.predict(X)) + \n            (0.1 * svr_model_full_data.predict(X)) + \n            (0.1 * gbr_model_full_data.predict(X)) + \n            (0.15 * xgb_model_full_data.predict(X)) + \n            (0.1 * lgb_model_full_data.predict(X)) + \n            (0.3 * stack_gen_model.predict(np.array(X))))\n\n\nprint('RMSLE score on train data:')\nprint(rmsle(y, blend_models_predict(X)))\nprint('MSE score on train data:')\nprint(mean_squared_error(y, blend_models_predict(X)))\nprint('MAE score on train data:')\nprint(mean_absolute_error(np.expm1(y), np.floor(np.expm1(blend_models_predict(X)))))\n\n\nprint('Predict submission', datetime.now(), )\nsubmission = pd.read_csv(\"/kaggle/input/home-data-for-ml-course/sample_submission.csv\")\n\nsubmission.iloc[:, 1] = np.floor(np.expm1(blend_models_predict(X_sub)))\n\nsubmission.to_csv(\"House_price_submission.csv\", index=False)\nprint('Save submission', datetime.now(), )","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true,"execution":{"iopub.status.busy":"2025-05-05T10:03:45.161215Z","iopub.execute_input":"2025-05-05T10:03:45.161526Z","iopub.status.idle":"2025-05-05T10:16:23.695676Z","shell.execute_reply.started":"2025-05-05T10:03:45.161503Z","shell.execute_reply":"2025-05-05T10:16:23.694729Z"}},"outputs":[{"name":"stdout","text":"['home-data-for-ml-course']\nTrain set size: (1460, 81)\nTest set size: (1459, 80)\nSTART data processing 2025-05-05 10:03:45.235503\n(2917, 79)\n","output_type":"stream"},{"name":"stderr","text":"/usr/local/lib/python3.11/dist-packages/scipy/stats/_morestats.py:1340: ConstantInputWarning: An input array is constant; the correlation coefficient is not defined.\n  r, prob = _stats_py.pearsonr(xvals, yvals)\n","output_type":"stream"},{"name":"stdout","text":"Box-Cox failed for column: LotArea, error: The algorithm terminated without finding a valid bracket. Consider trying different initial points.\nYeo-Johnson also failed for column: LotArea, error: name 'yeojohnson' is not defined\nSkipping transformation for column: LotArea\nBox-Cox failed for column: 1stFlrSF, error: The algorithm terminated without finding a valid bracket. Consider trying different initial points.\nYeo-Johnson also failed for column: 1stFlrSF, error: name 'yeojohnson' is not defined\nSkipping transformation for column: 1stFlrSF\n(2917, 89)\n(2917, 342)\nX (1458, 342) y (1458,) X_sub (1459, 342)\nX (1453, 339) y (1453,) X_sub (1459, 339)\nSTART ML 2025-05-05 10:03:45.888179\nTEST score on CV\nKernel Ridge score: 0.1041 (0.0147)\n 2025-05-05 10:04:27.629728\nLasso score: 0.1046 (0.0153)\n 2025-05-05 10:05:02.457394\nElasticNet score: 0.1048 (0.0152)\n 2025-05-05 10:07:45.017668\nSVR score: 0.1046 (0.0132)\n 2025-05-05 10:07:50.129259\nGradientBoosting score: 0.1069 (0.0139)\n 2025-05-05 10:09:56.254908\nLightgbm score: 0.1056 (0.0159)\n 2025-05-05 10:10:23.928491\n","output_type":"stream"},{"name":"stderr","text":"/usr/local/lib/python3.11/dist-packages/xgboost/core.py:160: UserWarning: [10:10:24] WARNING: /workspace/src/objective/regression_obj.cu:209: reg:linear is now deprecated in favor of reg:squarederror.\n  warnings.warn(smsg, UserWarning)\n/usr/local/lib/python3.11/dist-packages/xgboost/core.py:160: UserWarning: [10:10:31] WARNING: /workspace/src/objective/regression_obj.cu:209: reg:linear is now deprecated in favor of reg:squarederror.\n  warnings.warn(smsg, UserWarning)\n/usr/local/lib/python3.11/dist-packages/xgboost/core.py:160: UserWarning: [10:10:37] WARNING: /workspace/src/objective/regression_obj.cu:209: reg:linear is now deprecated in favor of reg:squarederror.\n  warnings.warn(smsg, UserWarning)\n/usr/local/lib/python3.11/dist-packages/xgboost/core.py:160: UserWarning: [10:10:44] WARNING: /workspace/src/objective/regression_obj.cu:209: reg:linear is now deprecated in favor of reg:squarederror.\n  warnings.warn(smsg, UserWarning)\n/usr/local/lib/python3.11/dist-packages/xgboost/core.py:160: UserWarning: [10:10:51] WARNING: /workspace/src/objective/regression_obj.cu:209: reg:linear is now deprecated in favor of reg:squarederror.\n  warnings.warn(smsg, UserWarning)\n/usr/local/lib/python3.11/dist-packages/xgboost/core.py:160: UserWarning: [10:10:59] WARNING: /workspace/src/objective/regression_obj.cu:209: reg:linear is now deprecated in favor of reg:squarederror.\n  warnings.warn(smsg, UserWarning)\n/usr/local/lib/python3.11/dist-packages/xgboost/core.py:160: UserWarning: [10:11:06] WARNING: /workspace/src/objective/regression_obj.cu:209: reg:linear is now deprecated in favor of reg:squarederror.\n  warnings.warn(smsg, UserWarning)\n/usr/local/lib/python3.11/dist-packages/xgboost/core.py:160: UserWarning: [10:11:13] WARNING: /workspace/src/objective/regression_obj.cu:209: reg:linear is now deprecated in favor of reg:squarederror.\n  warnings.warn(smsg, UserWarning)\n/usr/local/lib/python3.11/dist-packages/xgboost/core.py:160: UserWarning: [10:11:20] WARNING: /workspace/src/objective/regression_obj.cu:209: reg:linear is now deprecated in favor of reg:squarederror.\n  warnings.warn(smsg, UserWarning)\n/usr/local/lib/python3.11/dist-packages/xgboost/core.py:160: UserWarning: [10:11:29] WARNING: /workspace/src/objective/regression_obj.cu:209: reg:linear is now deprecated in favor of reg:squarederror.\n  warnings.warn(smsg, UserWarning)\n","output_type":"stream"},{"name":"stdout","text":"Xgboost score: 0.1077 (0.0162)\n 2025-05-05 10:11:35.425110\nSTART Fit\n2025-05-05 10:11:35.425349 StackingCVRegressor\n","output_type":"stream"},{"name":"stderr","text":"/usr/local/lib/python3.11/dist-packages/xgboost/core.py:160: UserWarning: [10:14:10] WARNING: /workspace/src/objective/regression_obj.cu:209: reg:linear is now deprecated in favor of reg:squarederror.\n  warnings.warn(smsg, UserWarning)\n/usr/local/lib/python3.11/dist-packages/xgboost/core.py:160: UserWarning: [10:14:14] WARNING: /workspace/src/objective/regression_obj.cu:209: reg:linear is now deprecated in favor of reg:squarederror.\n  warnings.warn(smsg, UserWarning)\n/usr/local/lib/python3.11/dist-packages/xgboost/core.py:160: UserWarning: [10:14:17] WARNING: /workspace/src/objective/regression_obj.cu:209: reg:linear is now deprecated in favor of reg:squarederror.\n  warnings.warn(smsg, UserWarning)\n/usr/local/lib/python3.11/dist-packages/xgboost/core.py:160: UserWarning: [10:14:21] WARNING: /workspace/src/objective/regression_obj.cu:209: reg:linear is now deprecated in favor of reg:squarederror.\n  warnings.warn(smsg, UserWarning)\n/usr/local/lib/python3.11/dist-packages/xgboost/core.py:160: UserWarning: [10:14:25] WARNING: /workspace/src/objective/regression_obj.cu:209: reg:linear is now deprecated in favor of reg:squarederror.\n  warnings.warn(smsg, UserWarning)\n/usr/local/lib/python3.11/dist-packages/xgboost/core.py:160: UserWarning: [10:14:42] WARNING: /workspace/src/objective/regression_obj.cu:209: reg:linear is now deprecated in favor of reg:squarederror.\n  warnings.warn(smsg, UserWarning)\n/usr/local/lib/python3.11/dist-packages/xgboost/core.py:160: UserWarning: [10:15:24] WARNING: /workspace/src/objective/regression_obj.cu:209: reg:linear is now deprecated in favor of reg:squarederror.\n  warnings.warn(smsg, UserWarning)\n","output_type":"stream"},{"name":"stdout","text":"2025-05-05 10:15:30.456282 elasticnet\n2025-05-05 10:15:46.421088 lasso\n2025-05-05 10:15:50.321850 ridge\n2025-05-05 10:15:53.559644 svr\n2025-05-05 10:15:54.106112 GradientBoosting\n2025-05-05 10:16:06.969172 xgboost\n","output_type":"stream"},{"name":"stderr","text":"/usr/local/lib/python3.11/dist-packages/xgboost/core.py:160: UserWarning: [10:16:07] WARNING: /workspace/src/objective/regression_obj.cu:209: reg:linear is now deprecated in favor of reg:squarederror.\n  warnings.warn(smsg, UserWarning)\n","output_type":"stream"},{"name":"stdout","text":"2025-05-05 10:16:15.309991 lightgbm\nRMSLE score on train data:\n0.05673190411872493\nMSE score on train data:\n0.0032185089449361993\nMAE score on train data:\n6952.448726772198\nPredict submission 2025-05-05 10:16:22.160250\nSave submission 2025-05-05 10:16:23.692057\n","output_type":"stream"}],"execution_count":6}]}